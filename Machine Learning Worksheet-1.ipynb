{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING\n",
    "WORKSHEET – 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Q1 to Q7, only one option is correct, Choose the correct option:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. The value of correlation coefficient will always be:\n",
    "A) between 0 and 1 B) greater than -1\n",
    "C) between -1 and 1 D) between 0 and -1\n",
    "\n",
    "2. Which of the following cannot be used for dimensionality reduction?\n",
    "A) Lasso Regularisation B) PCA\n",
    "C) Recursive feature elimination D) Ridge Regularisation\n",
    "\n",
    "3. Which of the following is not a kernel in Support Vector Machines?\n",
    "A) linear B) Radial Basis Function\n",
    "C) hyperplane D) polynomial\n",
    "\n",
    "4. Amongst the following, which one is least suitable for a dataset having non-linear decision boundaries?\n",
    "A) Logistic Regression B) Naïve Bayes Classifier\n",
    "C) Decision Tree Classifier D) Support Vector Classifier\n",
    "\n",
    "5. In a Linear Regression problem, ‘X’ is independent variable and ‘Y’ is dependent variable, where ‘X’ represents weight in pounds. If you convert the unit of ‘X’ to kilograms, then new coefficient of ‘X’ will be?\n",
    "(1 kilogram = 2.205 pounds)\n",
    "A) 2.205 × old coefficient of ‘X’ B) same as old coefficient of ‘X’\n",
    "C) old coefficient of ‘X’ ÷ 2.205 D) Cannot be determined\n",
    "\n",
    "6. As we increase the number of estimators in ADABOOST Classifier, what happens to the accuracy of the model?\n",
    "A) remains same B) increases\n",
    "C) decreases D) none of the above\n",
    "\n",
    "7. Which of the following is not an advantage of using random forest instead of decision trees?\n",
    "A) Random Forests reduce overfitting\n",
    "B) Random Forests explains more variance in data then decision trees\n",
    "C) Random Forests are easy to interpret\n",
    "D) Random Forests provide a reliable feature importance estimate\n",
    "\n",
    "In Q8 to Q10, more than one options are correct, Choose all the correct options:\n",
    "\n",
    "8. Which of the following are correct about Principal Components?\n",
    "A) Principal Components are calculated using supervised learning techniques\n",
    "B) Principal Components are calculated using unsupervised learning techniques\n",
    "C) Principal Components are linear combinations of Linear Variables.\n",
    "D) All of the above\n",
    "\n",
    "9. Which of the following are applications of clustering?\n",
    "A) Identifying developed, developing and under-developed countries on the basis of factors like GDP, poverty index, employment rate, population and living index\n",
    "B) Identifying loan defaulters in a bank on the basis of previous years’ data of loan accounts.\n",
    "C) Identifying spam or ham emails\n",
    "D) Identifying different segments of disease based on BMI, blood pressure, cholesterol, blood sugar levels.\n",
    "\n",
    "10. Which of the following is(are) hyper parameters of a decision tree?\n",
    "A) max_depth B) max_features\n",
    "C) n_estimators D) min_samples_leaf\n",
    "\n",
    "Q10 to Q15 are subjective answer type questions, Answer them briefly.\n",
    "\n",
    "11. What are outliers? Explain the Inter Quartile Range(IQR) method for outlier detection.\n",
    "\n",
    "12. What is the primary difference between bagging and boosting algorithms?\n",
    "\n",
    "13. What is adjusted R2 in logistic regression. How is it calculated?\n",
    "\n",
    "14. What is the difference between standardisation and normalisation?\n",
    "\n",
    "15. What is cross-validation? Describe one advantage and one disadvantage of using cross-validation."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. The value of correlation coefficient will always be:\n",
    "\n",
    "A) between 0 and 1 \n",
    "\n",
    "B) greater than -1\n",
    "\n",
    "C) between -1 and 1 \n",
    "\n",
    "D) between 0 and -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:-- C) between -1 and 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Which of the following cannot be used for dimensionality reduction?\n",
    "\n",
    "A) Lasso Regularisation \n",
    "\n",
    "B) PCA\n",
    "\n",
    "C) Recursive feature elimination \n",
    "\n",
    "D) Ridge Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:-- C) Recursive feature elimination"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "3. Which of the following is not a kernel in Support Vector Machines?\n",
    "\n",
    "A) linear \n",
    "\n",
    "B) Radial Basis Function\n",
    "\n",
    "C) hyperplane \n",
    "\n",
    "D) polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:-- C) hyperplane"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. Amongst the following, which one is least suitable for a dataset having non-linear decision boundaries?\n",
    "A) Logistic Regression B) Naïve Bayes Classifier\n",
    "C) Decision Tree Classifier D) Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:-- A) Logistic Regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5. In a Linear Regression problem, ‘X’ is independent variable and ‘Y’ is dependent variable, where ‘X’ represents weight in pounds. If you convert the unit of ‘X’ to kilograms, then new coefficient of ‘X’ will be?\n",
    "(1 kilogram = 2.205 pounds)\n",
    "A) 2.205 × old coefficient of ‘X’ B) same as old coefficient of ‘X’\n",
    "C) old coefficient of ‘X’ ÷ 2.205 D) Cannot be determined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:-- C) old coefficient of ‘X’ ÷ 2.205"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6. As we increase the number of estimators in ADABOOST Classifier, what happens to the accuracy of the model?\n",
    "A) remains same B) increases\n",
    "C) decreases D) none of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer:--"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7. Which of the following is not an advantage of using random forest instead of decision trees?\n",
    "A) Random Forests reduce overfitting\n",
    "B) Random Forests explains more variance in data then decision trees\n",
    "C) Random Forests are easy to interpret\n",
    "D) Random Forests provide a reliable feature importance estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:-- C) Random Forests are easy to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Q8 to Q10, more than one options are correct, Choose all the correct options:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "8. Which of the following are correct about Principal Components?\n",
    "A) Principal Components are calculated using supervised learning techniques\n",
    "B) Principal Components are calculated using unsupervised learning techniques\n",
    "C) Principal Components are linear combinations of Linear Variables.\n",
    "D) All of the above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:--\n",
    "    \n",
    "B) Principal Components are calculated using unsupervised learning techniques\n",
    "\n",
    "C) Principal Components are linear combinations of Linear Variables."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "9. Which of the following are applications of clustering?\n",
    "A) Identifying developed, developing and under-developed countries on the basis of factors like GDP, poverty index, employment rate, population and living index\n",
    "B) Identifying loan defaulters in a bank on the basis of previous years’ data of loan accounts.\n",
    "C) Identifying spam or ham emails\n",
    "D) Identifying different segments of disease based on BMI, blood pressure, cholesterol, blood sugar levels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:--  \n",
    "A) Identifying developed, developing and under-developed countries on the basis of factors like GDP, poverty index, employment rate, population and living index\n",
    "\n",
    "B) Identifying loan defaulters in a bank on the basis of previous years’ data of loan accounts.\n",
    "\n",
    "D) Identifying different segments of disease based on BMI, blood pressure, cholesterol, blood sugar levels."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "10. Which of the following is(are) hyper parameters of a decision tree?\n",
    "A) max_depth B) max_features\n",
    "C) n_estimators D) min_samples_leaf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:--\n",
    "    \n",
    "A) max_depth \n",
    "\n",
    "B) max_features\n",
    "\n",
    "D) min_samples_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10 to Q15 are subjective answer type questions, Answer them briefly."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "11. What are outliers? Explain the Inter Quartile Range(IQR) method for outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An observation which differs from an overall pattern on a sample dataset is called an outlier.\n",
    "\n",
    "The outliers may suggest experimental errors, variability in a measurement, or an anomaly.\n",
    "\n",
    "Significance of outliers:\n",
    "\n",
    "Outliers badly affect mean and standard deviation of the dataset. These may statistically give erroneous results.\n",
    "Most machine learning algorithms do not work well in the presence of outlier. So it is desirable to detect and remove outliers.\n",
    "Outliers are highly useful in anomaly detection like fraud detection where the fraud transactions are very different from normal transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR is used to measure variability by dividing a data set into quartiles. The data is sorted in ascending order and split into 4 equal parts. Q1, Q2, Q3 called first, second and third quartiles are the values which separate the 4 equal parts.\n",
    "\n",
    "Q1 represents the 25th percentile of the data.\n",
    "Q2 represents the 50th percentile of the data.\n",
    "Q3 represents the 75th percentile of the data.\n",
    "\n",
    "If a dataset has 2n / 2n+1 data points, then\n",
    "Q1 = median of the dataset.\n",
    "Q2 = median of n smallest data points.\n",
    "Q3 = median of n highest data points.\n",
    "\n",
    "IQR is the range between the first and the third quartiles namely Q1 and Q3: IQR = Q3 – Q1.\n",
    "\n",
    "The data points which fall below Q1 – 1.5 IQR or above Q3 + 1.5 IQR are outliers.\n",
    "\n",
    "Find the lower and upper limits as Q1 – 1.5 IQR and Q3 + 1.5 IQR\n",
    "\n",
    "Data points greater than the upper limit or less than the lower limit are outliers\n",
    "\n",
    "Q1 25 percentile of the given data \n",
    "Q2 50 percentile of the given data \n",
    "Q3 75 percentile of the given data \n",
    "Interquartile range\n",
    "\n",
    "Plot the box plot to highlight outliers.\n",
    "\n",
    "using::\n",
    "sns.boxplot(data)\n",
    "\n",
    "Conclusion: IQR and box plot are effective techniques to detect outliers in data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "12. What is the primary difference between bagging and boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is used when the goal is to reduce the variance of a decision tree classifier. Here the objective is to create several subsets of data from training sample chosen randomly with replacement. Each collection of subset data is used to train their decision trees. As a result, we get an ensemble of different models. Average of all the predictions from different trees are used which is more robust than a single decision tree classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is used to create a collection of predictors. In this technique, learners are learned sequentially with early learners fitting simple models to the data and then analysing data for errors. Consecutive trees (random sample) are fit and at every step, the goal is to improve the accuracy from the prior tree. When an input is misclassified by a hypothesis, its weight is increased so that next hypothesis is more likely to classify it correctly. This process converts weak learners into better performing model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "13. What is adjusted R2 in logistic regression. How is it calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R Square is a basic matrix which tells you about that how much variance is been explained by the model. What happens in a multivariate linear regression is that if you keep on adding new variables, the R square value will always increase irrespective of the variable significance. What adjusted R square do is calculate R square from only those variables whose addition in the model which are significant. So always while doing a multivariate linear regression we should look at adjusted R square instead of R square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n = Total observations\n",
    "\n",
    "p = Independent variables\n",
    "\n",
    "\n",
    "adj.r.squared = 1 - (1 - R.squared) * ((n - 1)/(n-p-1))\n",
    "\n",
    "\n",
    "print(adj.r.squared)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "14. What is the difference between standardisation and normalisation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most discussed scaling methods are Normalization and Standardization.\n",
    "\n",
    "Normalization typically means rescales the values into a range of [0,1].\n",
    "\n",
    "Standardization typically means rescales data to have a mean of 0 and a standard deviation of 1 (unit variance)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "15. What is cross-validation? Describe one advantage and one disadvantage of using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. Use cross-validation to detect overfitting, ie, failing to generalize a pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of Cross Validation\n",
    "\n",
    "1. Reduces Overfitting: In Cross Validation, we split the dataset into multiple folds and train the algorithm on different folds. This prevents our model from overfitting the training dataset. So, in this way, the model attains the generalization capabilities which is a good sign of a robust algorithm.\n",
    "\n",
    "Note: Chances of overfitting are less if the dataset is large. So, Cross Validation may not be required at all in the situation where we have sufficient data available.\n",
    "\n",
    "2. Hyperparameter Tuning: Cross Validation helps in finding the optimal value of hyperparameters to increase the efficiency of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantages of Cross Validation\n",
    "\n",
    "1. Increases Training Time: Cross Validation drastically increases the training time. Earlier you had to train your model only on one training set, but with Cross Validation you have to train your model on multiple training sets. \n",
    "\n",
    "For example, if you go with 5 Fold Cross Validation, you need to do 5 rounds of training each on different 4/5 of available data. And this is for only one choice of hyperparameters. If you have multiple choice of parameters, then the training period will shoot too high.\n",
    "\n",
    "2. Needs Expensive Computation: Cross Validation is computationally very expensive in terms of processing power required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
